{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-Generate a Summary from Long Youtube Videos Using AI\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*First we will import Whisper Model,pyTube to download youtube videos,tranformers model to create summary,list to deal with list data and logging to show the basic errors that may be there while creating the summary.</br>\n",
    "Whole Approach of getting the Youtube video summary is to first download the video using pytube library and then convert the youtube video in text format using Whishper model and then find the summary of that text content using BART model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "from pytube import YouTube\n",
    "from transformers import pipeline\n",
    "from typing import List\n",
    "import logging\n",
    "logging.basicConfig(filename='demo.log', encoding='utf-8', level=logging.ERROR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Give the URL of youtube video and a file name which you want to give to the video for your local system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.youtube.com/watch?v=p0Tfs7VNp7s\"\n",
    "VIDEO_NAME = \"demo\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now we will download the audio .mp3 format from the whole youtube video.We will use YouTube function from pytube library and save it with the file name given above and .mp3 format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_audio_from_youtube(url: str, video_name: str) -> str:\n",
    "    video_url = YouTube(url)\n",
    "    video = video_url.streams.filter(only_audio=True).first()\n",
    "    filename = video_name + \".mp3\"\n",
    "    video.download(filename=filename)\n",
    "    return filename"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now we will load the whisper model from whisper library then we will convert whole .mp3 file audio into text format using transcribe function given in whisper.We will create a .txt file and saves it in that txt file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_whisper_model(model_name: str = \"medium\"):\n",
    "    \"\"\"Load the medium multilingual Whisper model.\"\"\"\n",
    "    return whisper.load_model(model_name)\n",
    "\n",
    "\n",
    "def transcribe_audio_to_text(model, audio_path: str, language: str = \"English\"):\n",
    "    \"\"\"Transcribe the audio using the Whisper model.\"\"\"\n",
    "    return model.transcribe(audio_path, fp16=False, language=language)\n",
    "\n",
    "\n",
    "def save_text_to_file(text: str, file_name: str):\n",
    "    \"\"\"Save the transcribed text to a file.\"\"\"\n",
    "    try:\n",
    "        with open(file_name, \"w+\") as file:\n",
    "            file.write(text)\n",
    "    except (IOError, OSError, FileNotFoundError, PermissionError) as e:\n",
    "        logging.debug(f\"Error in file operation: {e}\")\n",
    "\n",
    "\n",
    "def get_text(url: str, video_name: str) -> None:\n",
    "    model = load_whisper_model()\n",
    "    audio_path = download_audio_from_youtube(url, video_name)\n",
    "    result = transcribe_audio_to_text(model, audio_path)\n",
    "    save_text_to_file(result[\"text\"], video_name + \".txt\")\n",
    "\n",
    "\n",
    "get_text(url=URL, video_name=VIDEO_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Whole text will be large in size so we will break it in form of small chuncks or tokens using nltk library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "def read_file(file_name: str) -> str:\n",
    "    try:\n",
    "        with open(file_name + \".txt\", \"r\", encoding=\"utf8\") as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"{e}: File '{file_name}.txt' not found.\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading file: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def split_text_into_chunks(document: str, max_tokens: int) -> List[str]:\n",
    "    if not document:\n",
    "        return []\n",
    "\n",
    "    chunks, current_chunk, current_length = [], [], 0\n",
    "\n",
    "    try:\n",
    "        for sentence in nltk.sent_tokenize(document):\n",
    "            sentence_length = len(sentence)\n",
    "\n",
    "            if current_length + sentence_length < max_tokens:\n",
    "                current_chunk.append(sentence)\n",
    "                current_length += sentence_length\n",
    "            else:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk, current_length = [sentence], sentence_length\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "        return chunks\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error splitting text into chunks: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "long_text = read_file(VIDEO_NAME)\n",
    "if long_text:\n",
    "    text_chunks = split_text_into_chunks(long_text, max_tokens=4000)\n",
    "    logging.info(f\"Text chunks: {text_chunks}\")\n",
    "else:\n",
    "    logging.error(\"Error: Unable to process the text.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here we will finally set the different parametres for BART model.We will create a pipeline model summarizer which will be passed to BART summarizer and It will give us the summary of whole text file in sort form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Dict\n",
    "bart_params = {\n",
    "    \"max_length\": 124,\n",
    "    \"min_length\": 30,\n",
    "    \"do_sample\": False,\n",
    "    \"truncation\": True,\n",
    "    \"repetition_penalty\": 10.0,\n",
    "}\n",
    "\n",
    "\n",
    "def create_summarizer(model: str) -> Callable:\n",
    "    summarizer = pipeline(\"summarization\", model=model)\n",
    "    return summarizer\n",
    "\n",
    "\n",
    "def get_summary_bart(list_chunks: List[str], summarizer: Callable, summarization_params: Dict[str, int]) -> str:\n",
    "    try:\n",
    "        summaries = [\n",
    "            summarizer(chunk, **summarization_params)[0][\"summary_text\"]\n",
    "            for chunk in list_chunks\n",
    "        ]\n",
    "        return \" \".join(summaries)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error generating summaries: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now We will save the whole summary of the youtube video in a file by using save_summary_to_file function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_summary_to_file(summary: str, file_name: str) -> None:\n",
    "    try:\n",
    "        with open(f\"{file_name}.txt\", \"a\") as fp:\n",
    "            fp.write(summary)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving summary to file: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Text_chunks are of length 4 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_chunks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now we will actually call all the functions for creating summarizer,getting summary and then saving summary to a file.If length of the summary of the video is greater than 5000 then we will split the whole sumamry in form of chunks and impliment the summary function again in small chuncks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume text_chunks is already defined and contains the chunks of text from the previous steps\n",
    "summarizer = create_summarizer(\"facebook/bart-large-cnn\")\n",
    "summary = get_summary_bart(text_chunks, summarizer, bart_params)\n",
    "save_summary_to_file(summary, f\"summary_{VIDEO_NAME}\")\n",
    "\n",
    "if len(summary) > 5000:\n",
    "    # If the summary is to long we can reapply the summarization function\n",
    "    text_chunks = split_text_into_chunks(summary, max_tokens=1000)\n",
    "    short_summary = get_summary_bart(text_chunks, summarizer, bart_params)\n",
    "    save_summary_to_file(short_summary, f\"short_summary_{VIDEO_NAME}\")\n",
    "    logging.info(\"Summary saved to file.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can also save the whole summary into a file named summary_demo.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'summary_demo.txt'  # Replace with the actual file path\n",
    "file = open(file_path, 'r')\n",
    "\n",
    "# Step 2: Read the contents of the file\n",
    "file_contents = file.read()\n",
    "\n",
    "# Step 3: Close the file\n",
    "file.close()\n",
    "\n",
    "# Now you can work with the contents of the file stored in the variable 'file_contents'\n",
    "print(file_contents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href = \"https://towardsdatascience.com/how-to-auto-generate-a-summary-from-long-youtube-videos-using-ai-a2a542b6698d\">For further reading and Support,You can use this article.<br>Thank you</a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
